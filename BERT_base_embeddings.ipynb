{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **BERT BASE**\n",
        "\n",
        "Finding Embeddings"
      ],
      "metadata": {
        "id": "hOb8a4P8nOsa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text"
      ],
      "metadata": {
        "id": "NyrcQGjvneyb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **BERT Preprocessing**\n",
        "\n",
        "tokenizing the text into tokens that BERT was trained on, adding special tokens (like [CLS], [SEP]), and creating attention masks."
      ],
      "metadata": {
        "id": "rbQj4385n2ay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_url = \"https://kaggle.com/models/tensorflow/bert/TensorFlow2/en-uncased-preprocess/3\""
      ],
      "metadata": {
        "id": "xYmhR1jNnx0Q"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_preprocess_model = hub.KerasLayer(preprocess_url)"
      ],
      "metadata": {
        "id": "59J0wZuloE2o"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = [\"Please subscribe to this newsletter\", \"You are being offered this job role\"]\n",
        "text_preprocessed = bert_preprocess_model(test)\n",
        "text_preprocessed.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KG9ptLJod4y",
        "outputId": "5b2c834a-ef28-4e86-999c-6b1aa3b2c876"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['input_word_ids', 'input_type_ids', 'input_mask'])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **OUTPUT Explanation**\n",
        "\n",
        "'input_type_ids', 'input_word_ids', and 'input_mask' these outputs are specifically formatted to meet the input requirements of BERT models.\n",
        "\n",
        "**'input_word_ids':**\n",
        "\n",
        "or token IDs. The preprocessing model first tokenizes the text into words or subwords (subword tokenization helps in dealing with out-of-vocabulary words for which BERT hasn't been explicitly trained). Each token or subword is then mapped to a unique integer ID.\n",
        "\n",
        "**'input_mask':**\n",
        "\n",
        "or attention mask. The purpose of the 'input_mask' is to provide the model with information about which parts of the input data are actual tokens and which parts are padding.\n",
        "\n",
        "The attention mask has a binary value (0 or 1):\n",
        "1 - real token that should be attended to.\n",
        "0 - padding.\n",
        "\n",
        "**'input_type_ids':**\n",
        "\n",
        "or segment IDs. The 'input_type_ids' signal to the model which part of the input belongs to sentence A and which part belongs to sentence B.\n",
        "\n",
        "0 - first sentence.\n",
        "1 - second sentence.\n",
        "\n"
      ],
      "metadata": {
        "id": "SQAMgm7irsFx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_preprocessed['input_mask'], text_preprocessed['input_type_ids'], text_preprocessed['input_word_ids']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSlrynk7rpCx",
        "outputId": "9317ee8b-0782-4a66-882f-ef26c8d6a73b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(2, 128), dtype=int32, numpy=\n",
              " array([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
              "       dtype=int32)>,\n",
              " <tf.Tensor: shape=(2, 128), dtype=int32, numpy=\n",
              " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
              "       dtype=int32)>,\n",
              " <tf.Tensor: shape=(2, 128), dtype=int32, numpy=\n",
              " array([[  101,  3531,  4942, 29234,  2000,  2023, 17178,   102,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0],\n",
              "        [  101,  2017,  2024,  2108,  3253,  2023,  3105,  2535,   102,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0]], dtype=int32)>)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **BERT BASE Pre-Trained Model**"
      ],
      "metadata": {
        "id": "etPM1QXZuSWB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_url = \"https://www.kaggle.com/models/tensorflow/bert/TensorFlow2/en-uncased-l-12-h-768-a-12/4\""
      ],
      "metadata": {
        "id": "X1NikUjgs24e"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model = hub.KerasLayer(encoder_url)"
      ],
      "metadata": {
        "id": "QzwRwL8RuWVv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_results = bert_model(text_preprocessed)"
      ],
      "metadata": {
        "id": "xB3NlbdauZNi"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_results.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxMcOKV0ugxt",
        "outputId": "4039bcb4-6481-433f-8cfd-879697af1f69"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['encoder_outputs', 'sequence_output', 'default', 'pooled_output'])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **OUTPUT Explanation**\n",
        "\n",
        "['encoder_outputs', 'sequence_output', 'pooled_output', 'default'] these are part of the output from a BERT model after it processes input text. Each of these keys provides a different type of output from the BERT layers, useful for various downstream tasks in NLP.\n",
        "\n",
        "1.\t**'encoder_outputs':**\n",
        "\n",
        "•\tThis key provides the outputs of each individual encoder (Transformer block) within the BERT model. This is useful for tasks that might benefit from accessing intermediate layers of the model, rather than just the final output.\n",
        "\n",
        "•\tIn BERT-base model, the encoder_outputs contains 12 items i.e. transformer blocks or layers of the model, each contributing to the hierarchical understanding of the input text at various levels of abstraction.\n",
        "\n",
        "Accessing the output of each individual layer can be highly beneficial for certain NLP tasks. For example, earlier layers might be better for tasks focused on the syntactic nuances of the text, while later layers might be more effective for tasks involving complex semantic understanding.\n"
      ],
      "metadata": {
        "id": "P0Tbe66QzmFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(bert_results['encoder_outputs'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIpM3xxRzyF5",
        "outputId": "dce7c3f0-4cc5-43e5-bd71-b27883fbd361"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_results['encoder_outputs'][-1] == bert_results['sequence_output']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daUpTvZhzywF",
        "outputId": "c1b44041-6edd-4466-b977-c974c037f9f7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 128, 768), dtype=bool, numpy=\n",
              "array([[[ True,  True,  True, ...,  True,  True,  True],\n",
              "        [ True,  True,  True, ...,  True,  True,  True],\n",
              "        [ True,  True,  True, ...,  True,  True,  True],\n",
              "        ...,\n",
              "        [ True,  True,  True, ...,  True,  True,  True],\n",
              "        [ True,  True,  True, ...,  True,  True,  True],\n",
              "        [ True,  True,  True, ...,  True,  True,  True]],\n",
              "\n",
              "       [[ True,  True,  True, ...,  True,  True,  True],\n",
              "        [ True,  True,  True, ...,  True,  True,  True],\n",
              "        [ True,  True,  True, ...,  True,  True,  True],\n",
              "        ...,\n",
              "        [ True,  True,  True, ...,  True,  True,  True],\n",
              "        [ True,  True,  True, ...,  True,  True,  True],\n",
              "        [ True,  True,  True, ...,  True,  True,  True]]])>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **'sequence_output':**\n",
        "\n",
        "The 'sequence_output' is the output from the last layer of the BERT model for each token in the input sequence. It provides a high-dimensional representation of each token in the context of the entire input sequence.\n",
        "\n",
        "The shape of the sequence_output tensor as (2, 128, 768) from a BERT model output indicates specific dimensions related to the model's processing of input text.\n",
        "\n",
        "**Batch Size (2):**\n",
        "\n",
        "indicates that the output is for a batch of two input sequences.\n",
        "\n",
        "**Sequence Length (128):**\n",
        "\n",
        "represents the sequence length, that is, the number of tokens (words or subwords) in each input sequence that the model processes. Each sequence is padded or truncated to this fixed length to ensure uniformity in input size, which is required for batch processing in neural networks.\n",
        "\n",
        "**Hidden Size (768): **\n",
        "\n",
        "it is the size of the hidden layers in the BERT model. This number indicates the dimensionality of the output vectors that BERT generates for each token in the input sequence. Each token's output vector is a 768-dimensional representation that encapsulates the contextual relationships learned by the model during training."
      ],
      "metadata": {
        "id": "U7yoiLLW2fhl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_results['sequence_output']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKMoBVmAz2hK",
        "outputId": "a3e4ef90-d7d3-4f15-ba8c-ff97bca70322"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 128, 768), dtype=float32, numpy=\n",
              "array([[[ 0.22257687, -0.10579046,  0.00254981, ..., -0.24145333,\n",
              "         -0.05483516,  0.30309677],\n",
              "        [ 0.29199606, -0.08624656, -0.50955796, ...,  0.0044163 ,\n",
              "          0.2859007 , -0.6242738 ],\n",
              "        [-0.25697884, -0.5023774 , -0.2705382 , ..., -0.1796462 ,\n",
              "         -0.32368812, -0.30876014],\n",
              "        ...,\n",
              "        [ 0.39664716,  0.07789326,  0.5355841 , ..., -0.04175995,\n",
              "         -0.43019056,  0.09946573],\n",
              "        [ 0.2147437 , -0.07457222,  0.5009455 , ...,  0.03458769,\n",
              "         -0.32633275, -0.07769995],\n",
              "        [ 0.3138128 , -0.03680493,  0.61024684, ..., -0.01120434,\n",
              "         -0.33619204, -0.08630194]],\n",
              "\n",
              "       [[ 0.16681583,  0.1201781 , -0.1994772 , ..., -0.1812833 ,\n",
              "          0.22992885,  0.2749213 ],\n",
              "        [-0.02964864, -0.14141597, -0.1265569 , ...,  0.0872253 ,\n",
              "          0.9378395 , -0.72198415],\n",
              "        [ 0.5813221 ,  0.10083358,  0.00974214, ..., -0.30160144,\n",
              "          0.49794245,  0.25141135],\n",
              "        ...,\n",
              "        [ 0.37730837, -0.08086479,  0.5346243 , ...,  0.23206377,\n",
              "          0.01457147, -0.12128704],\n",
              "        [ 0.32621604, -0.06952597,  0.5488193 , ...,  0.22620155,\n",
              "         -0.01608427, -0.07675062],\n",
              "        [ 0.33154255, -0.05472103,  0.5486798 , ...,  0.27399173,\n",
              "         -0.05168763, -0.04890089]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **'pooled_output':**\n",
        "\n",
        "The 'pooled_output' represents a fixed-length output vector for the entire input sequence and is usually derived from the hidden state of the first token of the sequence (which is the special [CLS] token in BERT). This token's final hidden state is typically used as the \"aggregate representation\" for classification tasks. It's processed through an additional dense layer with a Tanh activation function to generate the pooled output. This output is useful for classification tasks where the entire input sequence needs to be represented as a single fixed-size vector."
      ],
      "metadata": {
        "id": "sTep9MSX4JM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_results['pooled_output']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbjlJtmf3wpJ",
        "outputId": "202b6207-53e5-4685-e354-3d8be9509fcc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 768), dtype=float32, numpy=\n",
              "array([[-0.91949916, -0.45129243, -0.774078  , ..., -0.36198598,\n",
              "        -0.7211742 ,  0.880642  ],\n",
              "       [-0.8155011 , -0.2725284 ,  0.0147709 , ...,  0.1272465 ,\n",
              "        -0.56835717,  0.86130506]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. **default':**\n",
        "\n",
        "This output typically points to one of the other outputs as the default one that should be used for most tasks. In many implementations of BERT on TensorFlow Hub, the 'default' key points to the 'pooled_output' as it is the most commonly used output for various classification tasks. However, depending on the specific implementation or model variant, it might point to a different output."
      ],
      "metadata": {
        "id": "BNCnBGvp4kKR"
      }
    }
  ]
}